{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: how are you'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True) \n",
    "\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"how are you\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory \n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "  return_messages=True,\n",
    "  k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\": input}, {\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm) \n",
    "\n",
    "def add_message(input, output): \n",
    "  memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Sout Korea is so pretty\", \"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Nicolas from South Korea introduces himself to the AI, who expresses admiration for that fact and wishes it could visit the country because it is so pretty.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: Nicolas introduces himself as living in South Korea. The AI responds with enthusiasm, calling it cool and expressing a desire to visit because South Korea is so pretty.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm, \n",
    "  max_token_limit=10 , \n",
    "  return_message=True\n",
    ")\n",
    "\n",
    "def add_message(input, output): \n",
    "  memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"Sout Korea is so pretty\", \"I wish I could go!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm, \n",
    "  return_message=True\n",
    ")\n",
    "\n",
    "def add_message(input, output): \n",
    "  memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi I'm Nicolas, I live in South Korea\\nAI: Wow that is so cool!\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi I'm Nicolas, I live in South Korea\\nAI: Wow that is so cool!\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "You are a helpful AI talking to a human \n",
      "\n",
      "\n",
      "Human:My name is Nico\n",
      "You:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm, \n",
    "  max_token_limit=120, \n",
    "  memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\" \n",
    "You are a helpful AI talking to a human \n",
    "\n",
    "{chat_history}\n",
    "Human:{question}\n",
    "You:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm, \n",
    "  memory=memory, \n",
    "  prompt=PromptTemplate.from_template(template), \n",
    "  verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "  You are a helpful AI talking to a human \n",
      "\n",
      "  Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "  Human:I live in Seoul\n",
      "  You:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know, Nico! How can I assist you with information or support related to living in Seoul?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "You are a helpful AI talking to a human \n",
      "\n",
      "Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human:what is my name?\n",
      "You:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "You are a helpful AI talking to a human \n",
      "\n",
      "[]\n",
      "Human:My name is Nico\n",
      "You:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm, \n",
    "  max_token_limit=120, \n",
    "  memory_key=\"chat_history\", \n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "template = \"\"\" \n",
    "You are a helpful AI talking to a human \n",
    "\n",
    "{chat_history}\n",
    "Human:{question}\n",
    "You:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm, \n",
    "  memory=memory, \n",
    "  prompt=PromptTemplate.from_template(template), \n",
    "  verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['emoji', 'movie'] template='ì˜í™”ì œëª©: {movie}\\nì´ëª¨í‹°ì½˜: {emoji}\\n'\n",
      "ğŸ‡°ğŸ‡·âš”ï¸ğŸ–ï¸\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"movie\": \"íƒ‘ê±´\", \"emoji\": \"ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\"},\n",
    "    {\"movie\": \"ëŒ€ë¶€\", \"emoji\": \"ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"movie\", \"emoji\"], template=\"ì˜í™”ì œëª©: {movie}\\nì´ëª¨í‹°ì½˜: {emoji}\\n\"\n",
    ")\n",
    "\n",
    "print(example_prompt)\n",
    "\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"ë‹¤ìŒì€ ëª‡ ê°€ì§€ ì˜í™” ì œëª©ê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì„¸ ê°œì˜ ì´ëª¨í‹°ì½˜ì…ë‹ˆë‹¤:\",\n",
    "    suffix=\"\\nì˜í™” ì œëª©: {input}\\nì´ëª¨í‹°ì½˜:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# print(prompt.format(input=\"ì¸ì…‰ì…˜\"))\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=chat, memory=memory)\n",
    "\n",
    "response = chain.run({\"input\": \"íƒœê·¹ê¸° íœ˜ë‚ ë¦¬ë©°\"})\n",
    "print(response)  # ì˜ˆìƒ ì¶œë ¥: \"ğŸ’¤ğŸŒ€ğŸ•°ï¸\" ë˜ëŠ” ì´ì™€ ìœ ì‚¬í•œ ê²°ê³¼\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
